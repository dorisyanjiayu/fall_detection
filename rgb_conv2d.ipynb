{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import keypointrcnn_resnet50_fpn, KeypointRCNN_ResNet50_FPN_Weights\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URFallDataset(Dataset):\n",
    "    \"\"\" a sample of the dataset will be a selection of frames and name of zipfile\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, folders, transform=None, sampling=False, sample_len=50):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.sequences = folders\n",
    "        self.sampling = sampling\n",
    "        self.sample_len = sample_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        png_list = []\n",
    "        keypoint_list = []\n",
    "\n",
    "        path_name = os.path.join(self.root_dir, self.sequences[idx])\n",
    "        inf_list = os.listdir(path_name)\n",
    "        #print(inf_list)\n",
    "\n",
    "        if self.sampling: #indexing not great\n",
    "            interval = len(inf_list) // self.sample_len\n",
    "            #print(self.sequences[idx], \"total\", len(inf_list), \"interval\", interval)\n",
    "            start = len(inf_list) - interval * self.sample_len\n",
    "            for i in range(start, len(inf_list)):\n",
    "                if i % interval == 0:\n",
    "                    png = Image.open(path_name + \"/\" + inf_list[i])\n",
    "                    png_t = self.transform(png)\n",
    "                    png_list += [self.transform(png)]\n",
    "\n",
    "                    # outputs = keypoint_rcnn(k_transforms(png))\n",
    "                    # keypoints = outputs['keypoints']\n",
    "                    # print(keypoints.shape())\n",
    "                    # keypoint_list += [keypoints]\n",
    "\n",
    "        #print(\"png len\", len(png_list))\n",
    "        print(torch.cat(png_list, 0).shape)\n",
    "\n",
    "        # \"fall\" or \"adl-\"\n",
    "        fall_b = [1, 0] if self.sequences[idx][:4] == 'fall' else [0, 1]\n",
    "        sample = {\"img_list\": torch.cat(png_list, 0), \"category\": torch.tensor(fall_b)}\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_h(h_in, padding, dilation, kernel_size, stride):\n",
    "    return math.floor((h_in+2*padding-dilation*(kernel_size[0]-1)-1)/stride+1)\n",
    "\n",
    "def get_w(w_in, padding, dilation, kernel_size, stride):\n",
    "    return math.floor((w_in+2*padding-dilation*(kernel_size[1]-1)-1)/stride+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size, numLayer=1, numNeuron=[1024]):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        kernel_size = (3,3); stride = 4; padding = 2; dilation = 2; c_out = 3\n",
    "        h_in = size; w_in = size\n",
    "        h_out = get_h(h_in,padding,dilation,kernel_size,stride)\n",
    "        # w_out = get_w(h_in,padding,dilation,kernel_size,stride)\n",
    "        print(h_in, \"conv2d output size\", h_out, h_out)\n",
    "        kernel_size=(3,3); stride = 3;\n",
    "        h_out = get_h(h_out,0,1,(3,3),3)\n",
    "        print(\"Maxpooling output\", h_out)\n",
    "\n",
    "\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Conv2d(3, c_out, kernel_size, stride, padding, dilation),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3, 3),\n",
    "            # nn.Conv2d(3, c_out, kernel_size, stride, padding, dilation),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPool2d(3, 3),\n",
    "            # self.flatten(),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(c_out*h_out*h_out, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('imagetensor: {}'.format(x.shape))\n",
    "        # # x = self.flatten(x)\n",
    "        # # x = x.to(torch.float64)\n",
    "        # logits = self.linear_relu_stack(x)\n",
    "        # return logits\n",
    "\n",
    "        for layer in self.linear_relu_stack:\n",
    "            x = layer(x)\n",
    "            print(x.size())\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, sample in enumerate(dataloader): #?\n",
    "        target = sample['category']\n",
    "        img = sample['img_list']\n",
    "        # print(target, img)\n",
    "        print(\"batch\", batch)\n",
    "        # print(\"istensorimg\", torch.is_tensor(img))\n",
    "        # print(\"img tensor shape\", img.shape, \"target shape\", target.shape)\n",
    "        img = img.to(torch.float32)\n",
    "        # print(img)\n",
    "        pred = model(img)\n",
    "        pred_probab = nn.Softmax(dim=1)(pred)\n",
    "        # print(\"pred\", pred_probab, pred)\n",
    "        loss = loss_fn(pred_probab, target.to(torch.float16))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(img)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, tp, correct = 0, 0, 0\n",
    "    false_negative = 0\n",
    "    predicted_pos = 0\n",
    "\n",
    "    def is_fall(a):\n",
    "        return torch.argmax(pred) == 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "        for sample in dataloader:\n",
    "            img = sample['img_list']\n",
    "            target = sample['category']\n",
    "            img = img.to(torch.float32)\n",
    "            pred = model(img)\n",
    "            pred_probab = nn.Softmax(dim=1)(pred)\n",
    "            print(\"pred\", pred, pred_probab, torch.argmax(pred))\n",
    "\n",
    "            test_loss += loss_fn(pred_probab, target.to(torch.float16)).item()\n",
    "            tp += 1 if is_fall(pred_probab) and is_fall(target) else 0\n",
    "            correct += 1 if is_fall(pred_probab) == is_fall(target) else 0\n",
    "            #(torch.argmax(pred, 1) == target).type(torch.float).sum().item() \n",
    "            predicted_pos += 1 if is_fall(pred_probab) else 0\n",
    "            false_negative += 1 if not is_fall(pred_probab) and is_fall(target) else 0\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    precision = tp/predicted_pos if predicted_pos > 0 else 0\n",
    "    recall = tp/(tp+false_negative) if tp > 0  else 0\n",
    "    f1 = 2*precision*recall/(precision+recall) if tp > 0  else 0\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*tp/size):>0.1f}%, Avg loss: {test_loss:>8f}, Recall: {(100*recall):>0.1f}%, F1: {(100*f1):>0.1f}% \\n\")\n",
    "    return accuracy, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "49 21\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "torch.Size([3, 50, 50])\n",
      "sampling 0\n",
      "sampling 1\n",
      "sampling 2\n",
      "sampling 3\n",
      "sampling 4\n",
      "sampling 5\n",
      "sampling 6\n",
      "sampling 7\n",
      "sampling 8\n",
      "sampling 9\n",
      "sampling 10\n",
      "sampling 11\n",
      "sampling 12\n",
      "sampling 13\n",
      "sampling 14\n",
      "sampling 15\n",
      "sampling 16\n",
      "sampling 17\n",
      "sampling 18\n",
      "sampling 19\n",
      "sampling 20\n",
      "11 21 fall rate in test dset 0.5238095238095238\n"
     ]
    }
   ],
   "source": [
    "device = (\"cpu\")\n",
    "keypoint_rcnn = get_Krcnn()\n",
    "\n",
    "root_dir=\"/Users/dy/Documents/ODML/urf-rgb\"\n",
    "folders = os.listdir(root_dir)\n",
    "if \".DS_Store\" in folders:\n",
    "    folders.remove(\".DS_Store\")\n",
    "random.shuffle(folders)\n",
    "print(len(folders))\n",
    "train_size = int(len(folders)*0.7)\n",
    "\n",
    "train_seq = folders[:train_size]\n",
    "test_seq = folders[train_size:]\n",
    "\n",
    "sample_len = 1\n",
    "img_size = 50\n",
    "transform = transforms.Compose([transforms.PILToTensor(), \n",
    "                                transforms.Resize(size=[img_size,img_size])])\n",
    "\n",
    "urf_train = URFallDataset(root_dir, folders = train_seq, transform=transform, sampling=True, sample_len=sample_len)\n",
    "urf_test = URFallDataset(root_dir, folders = test_seq, transform=transform, sampling=True, sample_len=sample_len)\n",
    "\n",
    "train_dataloader = DataLoader(urf_train, batch_size=1, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(urf_test, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "print(len(urf_train), len(urf_test))\n",
    "\n",
    "total_pos = 0\n",
    "for i, sample in list(enumerate(urf_test)):\n",
    "    if torch.eq(sample[\"category\"], torch.tensor([1,0]))[0]:\n",
    "        total_pos += 1\n",
    "    print(\"sampling\", i)\n",
    "\n",
    "print(total_pos, len(test_seq), \"fall rate in test dset\", total_pos/len(test_seq))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500\n",
      "50 conv2d output size 13 13\n",
      "Maxpooling output 4\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Conv2d(3, 3, kernel_size=(3, 3), stride=(3, 3), padding=(2, 2), dilation=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=48, out_features=1024, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=1024, out_features=64, bias=True)\n",
      "    (7): ReLU()\n",
      "    (8): Linear(in_features=64, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "param count 115990\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "torch.Size([3, 50, 50])\n",
      "batch 0\n",
      "imagetensor: torch.Size([1, 3, 50, 50])\n",
      "torch.Size([1, 3, 17, 17])\n",
      "torch.Size([1, 3, 17, 17])\n",
      "torch.Size([1, 3, 5, 5])\n",
      "torch.Size([1, 3, 5, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (15x5 and 48x1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[91], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m start_t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 25\u001b[0m train_loop(train_dataloader, model, loss_fn, optimizer)\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtraining used\u001b[39m\u001b[39m\"\u001b[39m, time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_t, \u001b[39m\"\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m start_t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[88], line 13\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     12\u001b[0m \u001b[39m# print(img)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m pred \u001b[39m=\u001b[39m model(img)\n\u001b[1;32m     14\u001b[0m pred_probab \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)(pred)\n\u001b[1;32m     15\u001b[0m \u001b[39m# print(\"pred\", pred_probab, pred)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[87], line 39\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39m# # x = self.flatten(x)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m# # x = x.to(torch.float64)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# logits = self.linear_relu_stack(x)\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[39m# return logits\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlinear_relu_stack:\n\u001b[0;32m---> 39\u001b[0m     x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m     40\u001b[0m     \u001b[39mprint\u001b[39m(x\u001b[39m.\u001b[39msize())\n\u001b[1;32m     41\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (15x5 and 48x1024)"
     ]
    }
   ],
   "source": [
    "size = sample_len * img_size * img_size * 3 \n",
    "print(size)\n",
    "\n",
    "model = NeuralNetwork(size=img_size, numLayer=2, numNeuron=1000).to(device)\n",
    "print(model)\n",
    "print(\"param count\", parameter_count(model))\n",
    "# print(\"model flop count\", flop_count(model))\n",
    "\n",
    "flops = []\n",
    "accuracys = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "latency = []\n",
    "\n",
    "learning_rate = 1e-3\n",
    "epochs = 2\n",
    "\n",
    "# Initialize the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    start_t = time.time()\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    print(\"training used\", time.time() - start_t, \"s\")\n",
    "    \n",
    "    start_t = time.time()\n",
    "    (accuracy, recall, f1) = test_loop(test_dataloader, model, loss_fn)\n",
    "    print(\"inference latency\", (time.time() - start_t) * 1000 / len(urf_test), \"ms\")\n",
    "\n",
    "    #calculate metrics ##\n",
    "    flops += [flop_count(model)]\n",
    "    accuracys += [accuracy]\n",
    "    recalls += [recall]\n",
    "    f1s += [f1]\n",
    "    latency += [(time.time() - start_t)/10]\n",
    "\n",
    "print(sample_len, img_size)\n",
    "print(\"flops=\", flops, \"\\n\", \"accuracy, recall, f1=\", accuracys, recalls, f1s, \"\\n\", \"latency=\", latency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
