{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import keypointrcnn_resnet50_fpn, KeypointRCNN_ResNet50_FPN_Weights\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URFallDataset(Dataset):\n",
    "    \"\"\" a sample of the dataset will be a selection of frames and name of zipfile\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, folders, transform=None, sampling=False, sample_len=50):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.sequences = folders\n",
    "        self.sampling = sampling\n",
    "        self.sample_len = sample_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        png_list = []\n",
    "        keypoint_list = []\n",
    "\n",
    "        path_name = os.path.join(self.root_dir, self.sequences[idx])\n",
    "        inf_list = os.listdir(path_name)\n",
    "        #print(inf_list)\n",
    "\n",
    "        if self.sampling: #indexing not great\n",
    "            interval = len(inf_list) // self.sample_len\n",
    "            #print(self.sequences[idx], \"total\", len(inf_list), \"interval\", interval)\n",
    "            start = len(inf_list) - interval * self.sample_len\n",
    "            for i in range(start, len(inf_list)):\n",
    "                if i % interval == 0:\n",
    "                    png = Image.open(path_name + \"/\" + inf_list[i])\n",
    "                    png_t = self.transform(png)\n",
    "                    png_list += [self.transform(png)]\n",
    "\n",
    "                    # outputs = keypoint_rcnn(k_transforms(png))\n",
    "                    # keypoints = outputs['keypoints']\n",
    "                    # print(keypoints.shape())\n",
    "                    # keypoint_list += [keypoints]\n",
    "\n",
    "        #print(\"png len\", len(png_list))\n",
    "        print(torch.cat(png_list, 0).shape)\n",
    "\n",
    "        # \"fall\" or \"adl-\"\n",
    "        fall_b = [1, 0] if self.sequences[idx][:4] == 'fall' else [0, 1]\n",
    "        sample = {\"img_list\": torch.cat(png_list, 0), \"category\": torch.tensor(fall_b)}\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size, numLayer=1, numNeuron=[1024]):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(size, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('imagetensor: {}'.format(x.shape))\n",
    "        x = self.flatten(x)\n",
    "        # x = x.to(torch.float64)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, sample in enumerate(dataloader): #?\n",
    "        target = sample['category']\n",
    "        img = sample['img_list']\n",
    "        # print(target, img)\n",
    "        print(\"batch\", batch)\n",
    "        # print(\"istensorimg\", torch.is_tensor(img))\n",
    "        # print(\"img tensor shape\", img.shape, \"target shape\", target.shape)\n",
    "        img = img.to(torch.float32)\n",
    "        # print(img)\n",
    "        pred = model(img)\n",
    "        pred_probab = nn.Softmax(dim=1)(pred)\n",
    "        # print(\"pred\", pred_probab, pred)\n",
    "        loss = loss_fn(pred_probab, target.to(torch.float16))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(img)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, tp, correct = 0, 0, 0\n",
    "    false_negative = 0\n",
    "    predicted_pos = 0\n",
    "\n",
    "    def is_fall(a):\n",
    "        return torch.argmax(pred) == 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "        for sample in dataloader:\n",
    "            img = sample['img_list']\n",
    "            target = sample['category']\n",
    "            img = img.to(torch.float32)\n",
    "            pred = model(img)\n",
    "            pred_probab = nn.Softmax(dim=1)(pred)\n",
    "            print(\"pred\", pred, pred_probab, torch.argmax(pred))\n",
    "\n",
    "            test_loss += loss_fn(pred_probab, target.to(torch.float16)).item()\n",
    "            tp += 1 if is_fall(pred_probab) and is_fall(target) else 0\n",
    "            correct += 1 if is_fall(pred_probab) == is_fall(target) else 0\n",
    "            #(torch.argmax(pred, 1) == target).type(torch.float).sum().item() \n",
    "            predicted_pos += 1 if is_fall(pred_probab) else 0\n",
    "            false_negative += 1 if not is_fall(pred_probab) and is_fall(target) else 0\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    precision = tp/predicted_pos if predicted_pos > 0 else 0\n",
    "    recall = tp/(tp+false_negative) if tp > 0  else 0\n",
    "    f1 = 2*precision*recall/(precision+recall) if tp > 0  else 0\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*tp/size):>0.1f}%, Avg loss: {test_loss:>8f}, Recall: {(100*recall):>0.1f}%, F1: {(100*f1):>0.1f}% \\n\")\n",
    "    return accuracy, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "49 21\n"
     ]
    }
   ],
   "source": [
    "device = (\"cpu\")\n",
    "keypoint_rcnn = get_Krcnn()\n",
    "\n",
    "root_dir=\"/Users/dy/Documents/ODML/urf-rgb\"\n",
    "folders = os.listdir(root_dir)\n",
    "if \".DS_Store\" in folders:\n",
    "    folders.remove(\".DS_Store\")\n",
    "random.shuffle(folders)\n",
    "print(len(folders))\n",
    "train_size = int(len(folders)*0.7)\n",
    "\n",
    "train_seq = folders[:train_size]\n",
    "test_seq = folders[train_size:]\n",
    "\n",
    "sample_len = 10\n",
    "img_size = 50\n",
    "transform = transforms.Compose([transforms.PILToTensor(), \n",
    "                                transforms.Resize(size=[img_size,img_size])])\n",
    "\n",
    "urf_train = URFallDataset(root_dir, folders = train_seq, transform=transform, sampling=True, sample_len=sample_len)\n",
    "urf_test = URFallDataset(root_dir, folders = test_seq, transform=transform, sampling=True, sample_len=sample_len)\n",
    "\n",
    "train_dataloader = DataLoader(urf_train, batch_size=1, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(urf_test, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "print(len(urf_train), len(urf_test))\n",
    "\n",
    "total_pos = 0\n",
    "for i, sample in list(enumerate(urf_test)):\n",
    "    if torch.eq(sample[\"category\"], torch.tensor([1,0]))[0]:\n",
    "        total_pos += 1\n",
    "    print(\"sampling\", i)\n",
    "\n",
    "print(total_pos, len(test_seq), \"fall rate in test dset\", total_pos/len(test_seq))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75000\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=75000, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=2, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "param count 76804100\n",
      "this layer: 153598976\n",
      "this layer: 4094\n",
      "this layer: 2046\n",
      "model flop count 153605116\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "torch.Size([30, 50, 50])\n",
      "batch 0\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x2 and 512x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m start_t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m---> 25\u001b[0m train_loop(train_dataloader, model, loss_fn, optimizer)\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mtraining used\u001b[39m\u001b[39m\"\u001b[39m, time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_t, \u001b[39m\"\u001b[39m\u001b[39ms\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m start_t \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[18], line 13\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     11\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     12\u001b[0m \u001b[39m# print(img)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m pred \u001b[39m=\u001b[39m model(img)\n\u001b[1;32m     14\u001b[0m pred_probab \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSoftmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)(pred)\n\u001b[1;32m     15\u001b[0m \u001b[39m# print(\"pred\", pred_probab, pred)\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[17], line 17\u001b[0m, in \u001b[0;36mNeuralNetwork.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     15\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x)\n\u001b[1;32m     16\u001b[0m \u001b[39m# x = x.to(torch.float64)\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear_relu_stack(x)\n\u001b[1;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m logits\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x2 and 512x2)"
     ]
    }
   ],
   "source": [
    "size = sample_len * img_size * img_size * 3 \n",
    "print(size)\n",
    "\n",
    "model = NeuralNetwork(size=size, numLayer=2, numNeuron=1000).to(device)\n",
    "print(model)\n",
    "print(\"param count\", parameter_count(model))\n",
    "print(\"model flop count\", flop_count(model))\n",
    "\n",
    "flops = []\n",
    "accuracys = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "latency = []\n",
    "\n",
    "learning_rate = 1e-3\n",
    "epochs = 2\n",
    "\n",
    "# Initialize the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    start_t = time.time()\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    print(\"training used\", time.time() - start_t, \"s\")\n",
    "    \n",
    "    start_t = time.time()\n",
    "    (accuracy, recall, f1) = test_loop(test_dataloader, model, loss_fn)\n",
    "    print(\"inference latency\", (time.time() - start_t) * 1000 / len(urf_test), \"ms\")\n",
    "\n",
    "    #calculate metrics ##\n",
    "    flops += [flop_count(model)]\n",
    "    accuracys += [accuracy]\n",
    "    recalls += [recall]\n",
    "    f1s += [f1]\n",
    "    latency += [(time.time() - start_t)/10]\n",
    "\n",
    "print(sample_len, img_size)\n",
    "print(\"flops=\", flops, \"\\n\", \"accuracy, recall, f1=\", accuracys, recalls, f1s, \"\\n\", \"latency=\", latency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
