{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import keypointrcnn_resnet50_fpn, KeypointRCNN_ResNet50_FPN_Weights\n",
    "from collections import OrderedDict\n",
    "import matplotlib.pyplot as plt\n",
    "import zipfile\n",
    "from torchvision.io import read_image\n",
    "from PIL import Image\n",
    "import math\n",
    "import random\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "from utils import *\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "class URFallDataset(Dataset):\n",
    "    \"\"\" a sample of the dataset will be a selection of frames and name of zipfile\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir, folders, transform=None, sampling=False, sample_len=50):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.sequences = folders\n",
    "        self.sampling = sampling\n",
    "        self.sample_len = sample_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        png_list = []\n",
    "        keypoint_list = []\n",
    "\n",
    "        path_name = os.path.join(self.root_dir, self.sequences[idx])\n",
    "        inf_list = os.listdir(path_name)\n",
    "        #print(inf_list)\n",
    "\n",
    "        if self.sampling: #indexing not great\n",
    "            interval = len(inf_list) // self.sample_len\n",
    "            #print(self.sequences[idx], \"total\", len(inf_list), \"interval\", interval)\n",
    "            start = len(inf_list) - interval * self.sample_len\n",
    "            for i in range(start, len(inf_list)):\n",
    "                if i % interval == 0:\n",
    "                    png = Image.open(path_name + \"/\" + inf_list[i])\n",
    "                    png_t = self.transform(png)\n",
    "                    png_list += [self.transform(png)]\n",
    "\n",
    "                    # outputs = keypoint_rcnn(k_transforms(png))\n",
    "                    # keypoints = outputs['keypoints']\n",
    "                    # print(keypoints.shape())\n",
    "                    # keypoint_list += [keypoints]\n",
    "\n",
    "        #print(\"png len\", len(png_list))\n",
    "        print(torch.cat(png_list, 0).shape)\n",
    "\n",
    "        # \"fall\" or \"adl-\"\n",
    "        fall_b = [1, 0] if self.sequences[idx][:4] == 'fall' else [0, 1]\n",
    "        sample = {\"img_list\": torch.cat(png_list, 0), \"category\": torch.tensor(fall_b)}\n",
    "        return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, size, numLayer=1, numNeuron=[1024]):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        print('imagetensor: {}'.format(x.shape))\n",
    "        x = self.flatten(x)\n",
    "        # x = x.to(torch.float64)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, sample in enumerate(dataloader): #?\n",
    "        target = sample['category']\n",
    "        img = sample['img_list']\n",
    "        # print(target, img)\n",
    "        print(\"batch\", batch)\n",
    "        # print(\"istensorimg\", torch.is_tensor(img))\n",
    "        # print(\"img tensor shape\", img.shape, \"target shape\", target.shape)\n",
    "        img = img.to(torch.float32)\n",
    "        # print(img)\n",
    "        pred = model(img)\n",
    "        pred_probab = nn.Softmax(dim=1)(pred)\n",
    "        # print(\"pred\", pred_probab, pred)\n",
    "        loss = loss_fn(pred_probab, target.to(torch.float16))\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(img)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, tp, correct = 0, 0, 0\n",
    "    false_negative = 0\n",
    "    predicted_pos = 0\n",
    "\n",
    "    def is_fall(a):\n",
    "        return torch.argmax(pred) == 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "        for sample in dataloader:\n",
    "            img = sample['img_list']\n",
    "            target = sample['category']\n",
    "            img = img.to(torch.float32)\n",
    "            pred = model(img)\n",
    "            pred_probab = nn.Softmax(dim=1)(pred)\n",
    "            print(\"pred\", pred, pred_probab, torch.argmax(pred))\n",
    "\n",
    "            test_loss += loss_fn(pred_probab, target.to(torch.float16)).item()\n",
    "            tp += 1 if is_fall(pred_probab) and is_fall(target) else 0\n",
    "            correct += 1 if is_fall(pred_probab) == is_fall(target) else 0\n",
    "            #(torch.argmax(pred, 1) == target).type(torch.float).sum().item() \n",
    "            predicted_pos += 1 if is_fall(pred_probab) else 0\n",
    "            false_negative += 1 if not is_fall(pred_probab) and is_fall(target) else 0\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    accuracy = correct / size\n",
    "    precision = tp/predicted_pos if predicted_pos > 0 else 0\n",
    "    recall = tp/(tp+false_negative) if tp > 0  else 0\n",
    "    f1 = 2*precision*recall/(precision+recall) if tp > 0  else 0\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*tp/size):>0.1f}%, Avg loss: {test_loss:>8f}, Recall: {(100*recall):>0.1f}%, F1: {(100*f1):>0.1f}% \\n\")\n",
    "    return accuracy, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n",
      "49 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "sampling 0\n",
      "sampling 1\n",
      "sampling 2\n",
      "sampling 3\n",
      "sampling 4\n",
      "sampling 5\n",
      "sampling 6\n",
      "sampling 7\n",
      "sampling 8\n",
      "sampling 9\n",
      "sampling 10\n",
      "sampling 11\n",
      "sampling 12\n",
      "sampling 13\n",
      "sampling 14\n",
      "sampling 15\n",
      "sampling 16\n",
      "sampling 17\n",
      "sampling 18\n",
      "sampling 19\n",
      "sampling 20\n",
      "5 21 fall rate in test dset 0.23809523809523808\n"
     ]
    }
   ],
   "source": [
    "device = (\"cpu\")\n",
    "keypoint_rcnn = get_Krcnn()\n",
    "\n",
    "root_dir=\"/Users/dy/Documents/ODML/urf-rgb\"\n",
    "folders = os.listdir(root_dir)\n",
    "if \".DS_Store\" in folders:\n",
    "    folders.remove(\".DS_Store\")\n",
    "random.shuffle(folders)\n",
    "print(len(folders))\n",
    "train_size = int(len(folders)*0.7)\n",
    "\n",
    "train_seq = folders[:train_size]\n",
    "test_seq = folders[train_size:]\n",
    "\n",
    "sample_len = 10\n",
    "img_size = 50\n",
    "transform = transforms.Compose([transforms.PILToTensor(), \n",
    "                                transforms.Resize(size=[img_size,img_size])])\n",
    "\n",
    "urf_train = URFallDataset(root_dir, folders = train_seq, transform=transform, sampling=True, sample_len=sample_len)\n",
    "urf_test = URFallDataset(root_dir, folders = test_seq, transform=transform, sampling=True, sample_len=sample_len)\n",
    "\n",
    "train_dataloader = DataLoader(urf_train, batch_size=1, shuffle=True, num_workers=0)\n",
    "test_dataloader = DataLoader(urf_test, batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "print(len(urf_train), len(urf_test))\n",
    "\n",
    "total_pos = 0\n",
    "for i, sample in list(enumerate(urf_test)):\n",
    "    if torch.eq(sample[\"category\"], torch.tensor([1,0]))[0]:\n",
    "        total_pos += 1\n",
    "    print(\"sampling\", i)\n",
    "\n",
    "print(total_pos, len(test_seq), \"fall rate in test dset\", total_pos/len(test_seq))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=75000, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "this layer: 76799488\n",
      "this layer: 2046\n",
      "model flop count 76801534\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "torch.Size([30, 50, 50])\n",
      "batch 0\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "loss: 0.989787  [    1/   49]\n",
      "torch.Size([30, 50, 50])\n",
      "batch 1\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 2\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 3\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 4\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 5\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 6\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 7\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 8\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 9\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 10\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 11\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 12\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 13\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 14\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 15\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 16\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 17\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 18\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 19\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 20\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 21\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 22\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 23\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 24\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 25\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 26\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 27\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 28\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 29\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 30\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 31\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 32\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 33\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 34\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 35\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 36\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 37\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 38\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 39\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 40\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 41\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 42\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 43\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 44\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 45\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 46\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 47\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 48\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "training used 29.10666799545288 s\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-162807.5938,  156204.1719]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-158042.0938,  151613.1875]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-137102.5156,  131562.7656]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-155970.7031,  149631.7500]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-168407.6406,  161569.9688]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-171512.6719,  164537.5156]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-138227.2031,  132606.5625]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-157714.5781,  151310.0000]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-34427.8867,  33020.5781]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-144336.7344,  138460.1406]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-139573.0469,  133930.4375]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-169123.0000,  162260.0938]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-170563.0781,  163635.0000]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-145508.1406,  139578.2500]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-33685.8789,  32310.3242]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-146829.2969,  140878.1875]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-34883.0039,  33462.2656]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-37338.8242,  35811.4844]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-36017.1680,  34549.2617]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-171446.7344,  164483.4375]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-136106.8438,  130599.5625]]) tensor([[0., 1.]]) tensor(1)\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.551357, Recall: 0.0%, F1: 0.0% \n",
      "\n",
      "inference latency 240.04950977507093 ms\n",
      "this layer: 76799488\n",
      "this layer: 2046\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "torch.Size([30, 50, 50])\n",
      "batch 0\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "loss: 1.313262  [    1/   49]\n",
      "torch.Size([30, 50, 50])\n",
      "batch 1\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 2\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 3\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 4\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 5\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 6\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 7\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 8\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 9\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 10\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 11\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 12\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 13\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 14\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 15\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 16\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 17\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 18\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 19\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 20\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 21\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 22\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 23\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 24\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 25\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 26\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 27\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 28\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 29\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 30\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 31\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 32\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 33\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 34\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 35\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 36\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 37\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 38\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 39\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 40\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 41\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 42\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 43\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 44\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 45\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 46\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 47\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "torch.Size([30, 50, 50])\n",
      "batch 48\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "training used 26.87972617149353 s\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-170764.8594,  163850.7500]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-164388.2656,  157735.5625]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-34762.1133,  33344.3828]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-35221.6836,  33790.3789]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-146920.7656,  140946.8281]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-34012.9102,  32627.1562]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-157484.8594,  151098.6875]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-159576.4531,  153099.7031]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-37701.3281,  36162.6836]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-145737.9844,  139817.6875]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-137428.4375,  131879.9219]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-36366.8750,  34888.0703]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-173111.2656,  166096.0625]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-170042.6094,  163153.9062]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-140928.2969,  135243.4375]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-173177.7969,  166150.6875]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-138433.7969,  132852.5312]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-139569.1406,  133906.6719]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-148254.9531,  142259.3594]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-172219.0625,  165239.3750]]) tensor([[0., 1.]]) tensor(1)\n",
      "torch.Size([30, 50, 50])\n",
      "imagetensor: torch.Size([1, 30, 50, 50])\n",
      "pred tensor([[-159245.7031,  152793.3438]]) tensor([[0., 1.]]) tensor(1)\n",
      "Test Error: \n",
      " Accuracy: 0.0%, Avg loss: 0.551357, Recall: 0.0%, F1: 0.0% \n",
      "\n",
      "inference latency 238.3054324558803 ms\n",
      "this layer: 76799488\n",
      "this layer: 2046\n",
      "10 50\n",
      "flops= [76801534, 76801534] \n",
      " accuracy, recall, f1= [1.0, 1.0] [0, 0] [0, 0] \n",
      " latency= [0.5041121006011963, 0.5004539251327514]\n"
     ]
    }
   ],
   "source": [
    "size = sample_len * img_size * img_size * 3 \n",
    "print(size)\n",
    "\n",
    "model = NeuralNetwork(size=size, numLayer=2, numNeuron=1000).to(device)\n",
    "print(model)\n",
    "print(\"model flop count\", flop_count(model))\n",
    "\n",
    "flops = []\n",
    "accuracys = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "latency = []\n",
    "\n",
    "learning_rate = 1e-3\n",
    "epochs = 2\n",
    "\n",
    "# Initialize the loss function and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    start_t = time.time()\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    print(\"training used\", time.time() - start_t, \"s\")\n",
    "    \n",
    "    start_t = time.time()\n",
    "    (accuracy, recall, f1) = test_loop(test_dataloader, model, loss_fn)\n",
    "    print(\"inference latency\", (time.time() - start_t) * 1000 / len(urf_test), \"ms\")\n",
    "\n",
    "    #calculate metrics ##\n",
    "    flops += [flop_count(model)]\n",
    "    accuracys += [accuracy]\n",
    "    recalls += [recall]\n",
    "    f1s += [f1]\n",
    "    latency += [(time.time() - start_t)/10]\n",
    "\n",
    "print(sample_len, img_size)\n",
    "print(\"flops=\", flops, \"\\n\", \"accuracy, recall, f1=\", accuracys, recalls, f1s, \"\\n\", \"latency=\", latency)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
